This is the README file to the pet_projects_spark_connect. 
For a deep dive into connecting Apache Spark to a local machine, I would like to thank the practice on Youtube - from the channel Datalearn.

The practice was conducted in conjunction with the material prepared by Oleg Agapov on interacting with Apache Spark. 

Link to practices: https://www.youtube.com/watch?v=OfS5o8vz-O8&t=1468s

Link to Oleg Agapovs GITHUB: https://github.com/oleg-agapov 

I take great pride in the industry when I see projects like this, able to share a wealth of information with other professionals in the field and novice professionals. 

The repository stores an ipynb file of the project made in Jupyter on the local machine. The file also contains a basic ETL-pipeline for dataframe conversion. The dataframe is also included in the repository. 

This project covers two of the few data modules of the Apache Spark engine. Spark SQL is a module that allows you to execute SQL queries on data, combining relational processing with a procedural API. It also uses the Spark Core module, which uses specialized primitives for recurrent processing in RAM, which significantly speeds up multiple access to user data loaded into memory. 

In this pet project, modules such as Spark Streaming, Spark MLlib and GraphX are not affected. 

I plan to update the pet-project in the future to expand my knowledge in this product, I will be glad if this project will help you in learning Apache Spark. I would also appreciate your feedback. 
